---
layout: post
title: 'Spark与mr理想状态的比较'
date: 2017-09-01
author: 男孩
tags: spark
---

#### Spakr 与  mr 的速度比较

#### 基于安装的spark 和mr  速度

##### spark 基于内存 而 mr 基于磁盘 具体的区别执行流程先不进行说明

###### spark_wordcount
```css
import java.util.Date
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
object WordCount {
  def main(args: Array[String]): Unit = {
    val date: Date = new Date()
    val time_start: Long =date.getTime

    println("程序开始执行")

    val sparkConf = new SparkConf()
    val sc = new SparkContext(sparkConf)
    println("拿到读取文件，开始读取")
    val line: RDD[String] = sc.textFile(args(0))
    println("开始进行处理")
    line.map((_,1)).reduceByKey(_+_).saveAsTextFile(args(1))
    val time_end: Long =date.getTime
    println("处理完成 所用时间:"+(time_end-time_start))
  }
}

```
###### mr_wordcount
```css
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.util.Date;

public class WordCountT {
    public static void main(String[] args) {
        Date date =new Date();
        long start=date.getTime();
        Configuration config = new Configuration();
        try {
            FileSystem fs = FileSystem. get ( config );
            Job job = Job. getInstance ( config );
            job .setJarByClass(WordCountT. class );
            job .setMapperClass(Mapper. class );
            job .setReducerClass(Reduce. class );
            job .setMapOutputKeyClass(Text. class );
            job .setMapOutputValueClass(IntWritable. class );
            // 给 job 指定计算的输入数据  / usr /input/wc.txt
            FileInputFormat. setInputPaths ( job , new Path(args[0]));
            // 给 job 指定计算之后结果的输出目录，该目录不允许存在，如果存在， job 执行出错
            Path output = new Path( args[1] );
            if ( fs .exists( output )){
                fs .delete( output , true );
            }
            FileOutputFormat. setOutputPath ( job , output );
            boolean f = job .waitForCompletion( true );
            if ( f ){
                System. out .println( " 执行成功 " );
            }
        } catch (Exception e ) {
            e .printStackTrace();
        }
        long end=date.getTime();
        System.out.println("已经完成 已使用时间："+(end-start));
    }
}
class Mapper extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, IntWritable>{
    Text key1 = new Text();
    IntWritable intWritable = new IntWritable(1);
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line=value.toString();
        key1.set(line);
        context.write(key1,intWritable);
    }
}
class Reduce extends Reducer<Text, IntWritable, Text, IntWritable>{
    IntWritable intWritable = new IntWritable();
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum=0;
        for(IntWritable a:values){
            sum+=a.get();
        }
        intWritable.set(sum);
        context.write(key,intWritable);
    }
}
```
因为基于yarn的 ，看一下他们的执行时间（这块的话因为有一个mr 我没有指定reduce数量）
###### spark的执行时间
![](http://mgimg-ali.oss-cn-beijing.aliyuncs.com/project/spark/wordcount/spark.PNG)
#####mr的执行时间
![](http://mgimg-ali.oss-cn-beijing.aliyuncs.com/project/spark/wordcount/mr%20.PNG)

##### 公司也都开始使用spark了处理离线业务，mr的速度确实慢，但是也在于优化，整体而言 离线处理的话 基本都用spark 但是为了代码统一化，使用java编写。

这是闲的没事干，跑了一下数据，数据是30G数据 ，分布比较均匀的数据，所以是针对理想状态而言的。